# -*- coding: utf-8 -*-
"""plantclassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gX2NP0c42v6-KyUZmFIk8Tjzz9ihp6zJ

# Wild Edible Plant Classification Notebook

## Kaggle Setup and Dataset Preparation
"""

! pip install kaggle

! mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

! kaggle datasets download ryanpartridge01/wild-edible-plants/

! unzip wild-edible-plants.zip

"""## Dataset Distribution Bar Chart"""

# Get bar chart of images per subdirectory
bar_classes = []

classes = ([name for name in os.listdir('/content/dataset/resized/')
            if os.path.isdir(os.path.join('/content/dataset/resized/', name))]) # get all directories 
for plant in classes:
    contents = os.listdir(os.path.join('/content/dataset/resized/', plant)) # get list of contents
    bar_classes.append([plant, len(contents)])

bar_classes = [['knapweed', 500],
              ['calendula', 500],
              ['common_yarrow', 473],
              ['garlic_mustard', 409],
              ['daisy', 490],
              ['pickerelweed', 454],
              ['red_clover', 449],
              ['common_milkweed', 467],
              ['ramsons', 489],
              ['chicory', 500],
              ['alfalfa', 470],
              ['cow_parsley', 499], 
              ['ground_ivy', 408],
              ['dandelion', 500],
              ['geranium', 500],
              ['crimson_clover', 400],
              ['fireweed', 499],
              ['fennel', 452],
              ['mullein', 500],
              ['burdock', 460],
              ['meadowsweet', 455],
              ['cattail', 466], 
              ['chickweed', 488],
              ['coltsfoot', 500],
              ['borage', 500],
              ['cowslip', 441],
              ['gardenia', 499],
              ['harebell', 500],
              ['common_vetch', 451],
              ['crithmum_maritimum', 433],
              ['henbit', 500], 
              ['allium', 481], 
              ['common_mallow', 439],
              ['coneflower', 500],
              ['chive_blossom', 454]]

labels, ys = zip(*bar_classes)
xs = np.arange(len(labels)) 
width = 1

col = (np.random.random(), np.random.random(), np.random.random())

fig = plt.figure()                                                               
ax = fig.gca()  #get current axes
ax.bar(xs, ys, color=col)

ax.set_title("Distribution of Images Per Plant Class")
ax.set_xticks(xs)
ax.set_xticklabels(labels, rotation = 90, fontsize = 8)

plt.savefig('distribution.png')

"""### Dataset Preparation with Train/Validation Subdirectory Splitting"""

!pip install split-folders tqdm
import splitfolders  # or import split_folders

# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.
splitfolders.fixed("/content/dataset/resized", output="/content/dataset", seed=1337, fixed=(100), oversample=False, group_prefix=None) # default values

"""## Importing Python Libraries"""

#Import libraries
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.layers import Dense, Flatten, Activation, Dropout
from keras import backend as K
from keras.backend import expand_dims
import numpy as np
from keras.preprocessing import image
from keras.preprocessing.image import load_img, img_to_array
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import numpy as np
import matplotlib.pyplot as plt
from keras.models import load_model
from PIL import Image
from keras.preprocessing.image import image
from google.colab import files
from IPython.display import Image

"""## Defining Constants"""

#Dimensions of images
IMG_WIDTH, IMG_HEIGHT = 224, 224
TRAIN_DATA_DIR = '/content/dataset/train'
VALIDATION_DATA_DIR = '/content/dataset/val'
NB_TRAIN_SAMPLES = 10000
NB_VALIDATION_SAMPLES = 2500
EPOCHS = 25
BATCH_SIZE = 32

"""## Image Preparation and Augmentation

Images are prepared for machine learning by ensuring that all images are in the necessary input shape. Images in the training set are augmented to improve training efficiency.
"""

if K.image_data_format() == 'channels_first':
	input_shape = (3, IMG_WIDTH, IMG_HEIGHT)
else:
	input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)

train_datagen = ImageDataGenerator(
	rescale = 1. / 255,
	shear_range = 0.2,
	zoom_range = 0.2,
	horizontal_flip = True)

# Augmentation configuration for testing:
test_datagen = ImageDataGenerator(rescale = 1. / 255)

train_generator = train_datagen.flow_from_directory(
	TRAIN_DATA_DIR,
	target_size = (IMG_WIDTH, IMG_HEIGHT),
	batch_size = BATCH_SIZE,
	class_mode = 'categorical')

validation_generator = train_datagen.flow_from_directory(
	VALIDATION_DATA_DIR,
	target_size = (IMG_WIDTH, IMG_HEIGHT),
	batch_size = BATCH_SIZE,
	class_mode = 'categorical')

"""### Visualization of Image Augmentation and Preparation"""

# load sample image
img = keras.preprocessing.image.load_img('/content/dataset/resized/cattail/cattail150.jpg')
# convert to numpy array
data = img_to_array(img)
# expand dimension to one sample
samples = expand_dims(data, 0)
# create image data augmentation generator
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range = 0.2,
    height_shift_range= 0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')
# prepare iterator
it = datagen.flow(samples, batch_size=1)
# generate samples and plot
for i in range(9):
  # define subplot
  plt.subplot(330 + 1 + i)
  # generate batch of images
  batch = it.next()
  # convert to unsigned integers for viewing
  image = batch[0].astype('uint8')
  # plot raw pixel data
  plt.imshow(image)
# show the figure
plt.show()
# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale = 1. / 255,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True)

train_generator = train_datagen.flow_from_directory(
    '/content/dataset/resized/',
    target_size=(224, 224),
    batch_size=32,
    shuffle=True,
    seed=123,
    class_mode='categorical',
    subset='training') # set as training data
# Sample of images from dataset
class_dict = train_generator.class_indices
new_dict={}
for key, value in class_dict.items():
    new_dict[value] = key        
images,labels = next(train_generator) # sample batch from generator
plt.figure(figsize = (10, 10))
length = len(labels)
if length < 25:   #show maximum of 25 images
    r = length
else:
    r = 25
for i in range(r):
    plt.subplot(5, 5, i + 1)
    image = (images[i]+1 )/2
    plt.imshow(image)
    index=np.argmax(labels[i])
    class_name = new_dict[index]
    plt.title(class_name, fontsize=16)
    plt.axis('off')
plt.show()

"""## Building the Machine Learning Model with Transfer Learning"""

tf.get_logger().setLevel('ERROR')
base_model= tf.keras.applications.MobileNetV2(
    include_top=False,
    input_shape=(224,224,3),
    weights='imagenet')

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu', name='fci')(x)
x = Dropout(0.5)(x)

predictions = Dense(35, activation='softmax', name='predictions')(x)

model = Model(base_model.input, predictions)

for layer in base_model.layers:
        layer.trainable = False

model.compile(loss='categorical_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])

"""## Training the Machine Learning Model"""

filepath="/content/drive/MyDrive/weights.best.hdf5"

red_lr= ReduceLROnPlateau(monitor='val_accuracy',patience=3,verbose=1,factor=0.1)
checkpoint = ModelCheckpoint(filepath, 
                              monitor='val_accuracy', 
                              verbose=1, 
                              save_best_only=True, 
                              mode='max')
callbacks_list = [checkpoint, red_lr]

history = model.fit(
	train_generator,
	steps_per_epoch = NB_TRAIN_SAMPLES // BATCH_SIZE,
	epochs = EPOCHS,
	validation_data = validation_generator,
	validation_steps = NB_VALIDATION_SAMPLES // BATCH_SIZE,
  callbacks = callbacks_list)

model.save_weights('/content/drive/MyDrive/ediblemodel.h5')

"""## Visualization of Model Accuracy and Loss"""

def plot_accuracy():
  # list all data in history
  print(history.history.keys())
  # summarize history for accuracy
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()
def plot_loss():
  # summarize history for loss
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()

  val_loss, val_acc = model.evaluate(validation_generator, steps=50)
  print('Validation accuracy:', val_acc)

"""## Training in Blue, Validation in Orange"""

plot_accuracy()
plot_loss()

"""## Model Deployment

### Uploading and Classifying Images
"""

def image_app():
  #Import libraries
  import keras
  from keras.preprocessing.image import ImageDataGenerator
  from keras.models import Sequential, Model
  from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
  from keras.layers import Dense, Flatten, Activation, Dropout
  from keras import backend as K
  from keras.backend import expand_dims
  import numpy as np
  from keras.preprocessing import image
  from keras.preprocessing.image import load_img, img_to_array
  from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
  import tensorflow as tf
  import matplotlib.pyplot as plt
  import os
  import numpy as np
  import matplotlib.pyplot as plt
  from keras.models import load_model
  from PIL import Image
  from keras.preprocessing.image import image
  from google.colab import files
  from IPython.display import Image
  filename = '/content/' + name
  img_pred = keras.preprocessing.image.load_img(filename, target_size= (224, 224))
  img_pred = keras.preprocessing.image.img_to_array(img_pred)/255.
  img_pred = keras.backend.expand_dims(img_pred, axis = 0)

  ##### Test model
  model = load_model('/content/weights.best.hdf5')
  rslt = model.predict(img_pred)
  y = round(rslt[0][np.argmax(rslt)], 2)
  confidence = y * 100

  preds_classes = np.argmax(rslt, axis=-1)

  plant_classes = ['alfalfa', 
                  'allium', 
                  'borage', 
                  'burdock', 
                  'calendula', 
                  'cattail', 
                  'chickweed', 
                  'chicory', 
                  'chive_blossom', 
                  'coltsfoot', 
                  'common_mallow', 
                  'common_milkweed', 
                  'common_vetch', 
                  'common_yarrow', 
                  'coneflower', 
                  'cow_parsley', 
                  'cowslip', 
                  'crimson_clover', 
                  'crithmum_maritimum', 
                  'daisy',
                  'dandelion', 
                  'fennel', 
                  'fireweed', 
                  'gardenia', 
                  'garlic_mustard', 
                  'geranium', 
                  'ground_ivy', 
                  'harebell', 
                  'henbit', 
                  'knapweed', 
                  'meadowsweet', 
                  'mullein', 
                  'pickerelweed', 
                  'ramsons', 
                  'red_clover']

  score = tf.nn.softmax(rslt[0])

  display(Image(filename, width = 150, height = 150))
  print(
      "\nThis is most likely an image of the wild edible plant {0} with a confidence of {1}%."
      .format(plant_classes[np.argmax(score)], confidence.round(2))
      )

"""# **Upload Image Below for Classification**

To begin this demonstration, sign up for or log into a Google account.

Download the setup.zip file from https://github.com/lefthandedcoder/plantclassifier.

Unzip the files.

Upload the weights.best.hdf5 using the upload icon under Files on the left. This may take several minutes.

After the hdf5 file has been uploaded and appears in the file directory on the left, upload an image using the button below.

Click on the arrows on the left of the following sections of code to run the program. When prompted, click 'Run anyway'.

Additional test images may also be uploaded and tested at this time.
"""

def image_app():
  #Import libraries
  import keras
  from keras.preprocessing.image import ImageDataGenerator
  from keras.models import Sequential, Model
  from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
  from keras.layers import Dense, Flatten, Activation, Dropout
  from keras import backend as K
  from keras.backend import expand_dims
  import numpy as np
  from keras.preprocessing import image
  from keras.preprocessing.image import load_img, img_to_array
  from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
  import tensorflow as tf
  import matplotlib.pyplot as plt
  import os
  import numpy as np
  import matplotlib.pyplot as plt
  from keras.models import load_model
  from PIL import Image
  from keras.preprocessing.image import image
  from google.colab import files
  from IPython.display import Image
  filename = '/content/' + name
  img_pred = keras.preprocessing.image.load_img(filename, target_size= (224, 224))
  img_pred = keras.preprocessing.image.img_to_array(img_pred)/255.
  img_pred = keras.backend.expand_dims(img_pred, axis = 0)

  ##### Test model
  model = load_model('/content/weights.best.hdf5')
  rslt = model.predict(img_pred)
  y = round(rslt[0][np.argmax(rslt)], 2)
  confidence = y * 100

  preds_classes = np.argmax(rslt, axis=-1)

  plant_classes = ['alfalfa', 
                  'allium', 
                  'borage', 
                  'burdock', 
                  'calendula', 
                  'cattail', 
                  'chickweed', 
                  'chicory', 
                  'chive_blossom', 
                  'coltsfoot', 
                  'common_mallow', 
                  'common_milkweed', 
                  'common_vetch', 
                  'common_yarrow', 
                  'coneflower', 
                  'cow_parsley', 
                  'cowslip', 
                  'crimson_clover', 
                  'crithmum_maritimum', 
                  'daisy',
                  'dandelion', 
                  'fennel', 
                  'fireweed', 
                  'gardenia', 
                  'garlic_mustard', 
                  'geranium', 
                  'ground_ivy', 
                  'harebell', 
                  'henbit', 
                  'knapweed', 
                  'meadowsweet', 
                  'mullein', 
                  'pickerelweed', 
                  'ramsons', 
                  'red_clover']

  score = tf.nn.softmax(rslt[0])

  display(Image(filename, width = 224, height = 224))
  print(
      "\nThis is most likely an image of the wild edible plant {0} with a confidence of {1}%."
      .format(plant_classes[np.argmax(score)], confidence.round(2))
      )

import ipywidgets as widgets
    
print('\nPlease upload an image for classification.\n')
uploader = widgets.FileUpload()
uploader

import io
from PIL import Image

for name, file_info in uploader.value.items():
    img = Image.open(io.BytesIO(file_info['content']))
    img.save(name)
image_app()