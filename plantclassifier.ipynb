{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plantclassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1D5EeFt6IiVgOTkbxRzBXM5mI9FaKa5s6",
      "authorship_tag": "ABX9TyMTBEDpV6qcpBqX9R14L7+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lefthandedcoder/plantclassifier/blob/main/plantclassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qo-LTcxAGbN",
        "outputId": "4e561861-847d-4371-c433-858cfc346b2a"
      },
      "source": [
        "! pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZkp9sOAMyB"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS--hLyyAPz3"
      },
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZONvHrOAUBv",
        "outputId": "5bfd14e2-cb99-4dec-d1f4-2503af3222ca"
      },
      "source": [
        "! kaggle datasets download ryanpartridge01/wild-edible-plants/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wild-edible-plants.zip to /content\n",
            " 99% 1.70G/1.70G [00:15<00:00, 137MB/s]\n",
            "100% 1.70G/1.70G [00:16<00:00, 114MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUveSoMeArTS"
      },
      "source": [
        "! unzip wild-edible-plants.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHtGXTCVTyPL"
      },
      "source": [
        "#import the libraries\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy.random import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owe5GkmUAk8"
      },
      "source": [
        "#Set the image size with are learning from\n",
        "IMG_WIDTH, IMG_HEIGHT = 224,224\n",
        "\n",
        "#Set the constants\n",
        "TRAIN_DATA_DIR = '/content/dataset/resized'\n",
        "\n",
        "EPOCHS = 25 #Higher for more time training model... diminishing returns\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Machine Learning Model Filename\n",
        "ML_MODEL_FILENAME = 'saved_model_mobilenet.h5'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FpHDpmYT5zN"
      },
      "source": [
        "def build_model():\n",
        "    \n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n",
        "    else:\n",
        "        input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    pretrained_model= tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        input_shape=(224,224,3),\n",
        "        pooling='avg',classes=35,\n",
        "        weights='imagenet')\n",
        "\n",
        "    for layer in pretrained_model.layers:\n",
        "        layer.trainable=False\n",
        "        \n",
        "    model.add(pretrained_model)    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(35, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmH3qvCFA2R3"
      },
      "source": [
        "def train_model(model):\n",
        "    # this is the augmentation configuration we will use for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      zoom_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,  \n",
        "      fill_mode='nearest',\n",
        "      validation_split=0.1) # set validation split\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR,\n",
        "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        class_mode='categorical',\n",
        "        subset='training') # set as training data\n",
        "    \n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DATA_DIR, # same directory as training data\n",
        "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        seed=123,\n",
        "        class_mode='categorical',\n",
        "        subset='validation') # set as validation data\n",
        "    \n",
        "    filepath=\"/content/drive/MyDrive/weights.best.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, \n",
        "                                 monitor='val_accuracy', \n",
        "                                 verbose=1, \n",
        "                                 save_best_only=True, \n",
        "                                 mode='max')\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = train_generator.samples // BATCH_SIZE,\n",
        "        validation_data = validation_generator, \n",
        "        validation_steps = validation_generator.samples // BATCH_SIZE,\n",
        "        epochs = EPOCHS, callbacks=callbacks_list)\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)\n",
        "    print('Validation accuracy:', val_acc)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-gxeRTIKIC"
      },
      "source": [
        "def get_visualizations():\n",
        "\n",
        "  # load sample image\n",
        "  img = load_img('/content/dataset/resized/cattail/cattail150.jpg')\n",
        "  # convert to numpy array\n",
        "  data = img_to_array(img)\n",
        "  # expand dimension to one sample\n",
        "  samples = expand_dims(data, 0)\n",
        "  # create image data augmentation generator\n",
        "  datagen = ImageDataGenerator(\n",
        "      rotation_range=20,\n",
        "      width_shift_range = 0.2,\n",
        "      height_shift_range= 0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "  # prepare iterator\n",
        "  it = datagen.flow(samples, batch_size=1)\n",
        "  # generate samples and plot\n",
        "  for i in range(9):\n",
        "    # define subplot\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    # generate batch of images\n",
        "    batch = it.next()\n",
        "    # convert to unsigned integers for viewing\n",
        "    image = batch[0].astype('uint8')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(image)\n",
        "  # show the figure\n",
        "  plt.show()\n",
        "  # this is the augmentation configuration we will use for training\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range = 0.2,\n",
        "      height_shift_range= 0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest',        \n",
        "      validation_split=0.1) # set validation split\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      TRAIN_DATA_DIR,\n",
        "      target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=True,\n",
        "      seed=123,\n",
        "      class_mode='categorical',\n",
        "      subset='training') # set as training data\n",
        "  # Sample of images from dataset\n",
        "  class_dict = train_generator.class_indices\n",
        "  new_dict={}\n",
        "  for key, value in class_dict.items():\n",
        "      new_dict[value] = key        \n",
        "  images,labels = next(train_generator) # sample batch from generator\n",
        "  plt.figure(figsize = (10, 10))\n",
        "  length = len(labels)\n",
        "  if length < 25:   #show maximum of 25 images\n",
        "      r = length\n",
        "  else:\n",
        "      r = 25\n",
        "  for i in range(r):\n",
        "      plt.subplot(5, 5, i + 1)\n",
        "      image = (images[i]+1 )/2\n",
        "      plt.imshow(image)\n",
        "      index=np.argmax(labels[i])\n",
        "      class_name = new_dict[index]\n",
        "      plt.title(class_name, fontsize=16)\n",
        "      plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZU0Dj0UKxpR"
      },
      "source": [
        "def get_bar():\n",
        "  # Get bar chart of images per subdirectory\n",
        "  import os\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plant_classes = []\n",
        "\n",
        "  classes = ([name for name in os.listdir('/content/dataset/resized/')\n",
        "              if os.path.isdir(os.path.join('/content/dataset/resized/', name))]) # get all directories \n",
        "  for plant in classes:\n",
        "      contents = os.listdir(os.path.join(TRAIN_DATA_DIR, plant)) # get list of contents\n",
        "      plant_classes.append([plant, len(contents)])\n",
        "      \n",
        "  labels, ys = zip(*plant_classes)\n",
        "  xs = np.arange(len(labels)) \n",
        "  width = 1\n",
        "\n",
        "  col = (np.random.random(), np.random.random(), np.random.random())\n",
        "\n",
        "  fig = plt.figure()                                                               \n",
        "  ax = fig.gca()  #get current axes\n",
        "  ax.bar(xs, ys, color=col)\n",
        "\n",
        "  ax.set_title(\"Distribution of Images Per Plant Class\")\n",
        "  ax.set_xticks(xs)\n",
        "  ax.set_xticklabels(labels, rotation = 90, fontsize = 8)\n",
        "\n",
        "  plt.savefig('distribution.png')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW7pWvexUHlX"
      },
      "source": [
        "def main():\n",
        "    myModel = None\n",
        "    tf.keras.backend.clear_session()\n",
        "    myModel = build_model()\n",
        "    SVG(model_to_dot(myModel).create(prog='dot', format='svg'))\n",
        "    myModel.summary()\n",
        "    myModel = train_model(myModel)\n",
        "    myModel.save(ML_MODEL_FILENAME)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs8tOD-aUI5q",
        "outputId": "3808d776-8d0e-47ee-d3ac-a11e707f18cf"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 35)                44835     \n",
            "=================================================================\n",
            "Total params: 2,302,819\n",
            "Trainable params: 44,835\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n",
            "Found 14885 images belonging to 35 classes.\n",
            "Found 1641 images belonging to 35 classes.\n",
            "Epoch 1/25\n",
            "31/58 [===============>..............] - ETA: 2:02 - loss: 3.3139 - accuracy: 0.1575"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKgZM8tUI22x"
      },
      "source": [
        "get_visualizations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPDUSMNCK3BO"
      },
      "source": [
        "get_bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpcBmE2CpIOA"
      },
      "source": [
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "from keras.preprocessing.image import image\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from IPython.display import Image \n",
        "\n",
        "plant_classes = []\n",
        "\n",
        "classes = ([name for name in os.listdir('/content/dataset/resized/')\n",
        "            if os.path.isdir(os.path.join('/content/dataset/resized/', name))]) # get all directories \n",
        "for plant in classes:\n",
        "    contents = os.listdir(os.path.join(TRAIN_DATA_DIR, plant)) # get list of contents\n",
        "    plant_classes.append([plant, len(contents)])\n",
        "\n",
        "  \n",
        "UPLOAD_FOLDER = \"content\"\n",
        "myModel = load_model('/content/drive/MyDrive/weights.best.hdf5')\n",
        "\n",
        "uploaded = files.upload()\n",
        "#filename = next(iter(uploaded))\n",
        "filename = '/content/Garlic_Mustard_close_800.jpg'\n",
        "Image(filename=filename, width = 224, height = 224)\n",
        "test_image = image.load_img('/content/Garlic_Mustard_close_800.jpg', target_size=(224, 224))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "predictions = myModel.predict(test_image)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This is most likely an image of the wild edible plant {} with a {:.2f}% confidence.\"\n",
        "    .format(plant_classes[np.argmax(score)][0], 100 * np.max(score))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}